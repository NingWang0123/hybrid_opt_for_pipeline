{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-D3d5gRD6GP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### code for he method\n",
        "\n",
        "\n",
        "def nonconvex_f(beta, X, y):\n",
        "    residual = y - X * beta  # Element-wise multiplication\n",
        "    residual_sq = residual ** 2\n",
        "    cost = 0.5 * np.sum(residual_sq / (1 + residual_sq))\n",
        "    return cost\n",
        "\n",
        "def nonconvex_grad_f(beta, X, y):\n",
        "    residual = y - X * beta\n",
        "    residual_sq = residual ** 2\n",
        "    factor = residual / (1 + residual_sq)\n",
        "    gradient = -np.sum(X * factor)\n",
        "    return gradient\n",
        "\n",
        "def nonconvex_hessian_f(beta, X, y):\n",
        "    residual = y - X * beta\n",
        "    residual_sq = residual ** 2\n",
        "    numerator = (1 - residual_sq)\n",
        "    denominator = (1 + residual_sq) ** 2\n",
        "    second_derivative = np.sum(X ** 2 * numerator / denominator)\n",
        "    H = np.array([[second_derivative]])\n",
        "    return H\n",
        "\n",
        "def sgd_with_bound(f, grad_f, start_point, end_point, X, y,\n",
        "                   learning_rate=0.01, iterations=1000, tol=1e-6):\n",
        "    x = np.array(start_point, dtype=float)\n",
        "    x_end = np.array(end_point, dtype=float)\n",
        "    for i in range(iterations):\n",
        "        gradient = grad_f(x, X, y)\n",
        "        x_new = x - learning_rate * gradient\n",
        "        if np.all(x_new >= x_end):\n",
        "            break\n",
        "        if np.linalg.norm(x_new - x) < tol:\n",
        "            break\n",
        "        x = x_new\n",
        "    return x, f(x, X, y)\n",
        "\n",
        "def rearrange_bounds(bounds):\n",
        "    min_bounds, max_bounds = bounds\n",
        "    rearranged = list(zip(min_bounds, max_bounds))\n",
        "    return rearranged\n",
        "\n",
        "def de_search_method3(f, bounds, X, y, maxiters=100):\n",
        "    result = differential_evolution(\n",
        "        lambda beta: f(beta, X, y),\n",
        "        rearrange_bounds(bounds),\n",
        "        maxiter=maxiters\n",
        "    )\n",
        "    return result.x, result.fun\n",
        "\n",
        "def generate_uniform_start_end_pairs(start_point, end_point, n):\n",
        "    points_lst = []\n",
        "    start = np.array(start_point)\n",
        "    end = np.array(end_point)\n",
        "    points = [start + t * (end - start) for t in np.linspace(0, 1, n)]\n",
        "    for i in range(n - 1):\n",
        "        start_pt = points[i]\n",
        "        end_pt = points[i + 1]\n",
        "        points_lst.append([start_pt, end_pt])\n",
        "    return points_lst\n",
        "\n",
        "def process_point_with_no_zoom_in(f, grad_f, hessian_f, global_search, pt, X, y,\n",
        "                                  learning_rate=0.01, iterations=1000, tol=1e-6):\n",
        "    points = []\n",
        "    results = []\n",
        "    start, end = pt\n",
        "    point, result = sgd_with_bound(f, grad_f, start, end, X, y, learning_rate, iterations, tol)\n",
        "    H = hessian_f(point, X, y)\n",
        "    eigenvalues = np.linalg.eigvalsh(H)\n",
        "    is_convex = np.all(eigenvalues >= 0)\n",
        "    points.append(point)\n",
        "    results.append(result)\n",
        "\n",
        "    if not is_convex:\n",
        "        ds_point, ds_min = global_search(f, ([-100], [100]), X, y, maxiters=100)\n",
        "        points.append(ds_point)\n",
        "        results.append(ds_min)\n",
        "\n",
        "    # print(f\"Found point: {point} with result: {result}\")\n",
        "    return points, results\n",
        "\n",
        "executor = ProcessPoolExecutor()\n",
        "\n",
        "def sgd_opt_global_search(start_intervals, end_intervals, n, f, grad_f, hessian_f, global_search,\n",
        "                          X, y, learning_rate=0.01, max_iterations=1000, tol=1e-6):\n",
        "    iters = int(max_iterations / n)\n",
        "    points_lst = generate_uniform_start_end_pairs(start_intervals, end_intervals, n)\n",
        "    futures = [executor.submit(\n",
        "        process_point_with_no_zoom_in, f, grad_f, hessian_f,\n",
        "        global_search, pt, X, y, learning_rate, iters, tol) for pt in points_lst]\n",
        "    return futures"
      ],
      "metadata": {
        "id": "fj52Df_JEVl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# convex with beta tuning\n",
        "# -----------------------------------------------------------\n",
        "# Full Pipeline with SGD to learn beta\n",
        "# -----------------------------------------------------------\n",
        "class FullPipeline:\n",
        "    def __init__(self, mean_val, median_val, num_categories=3):\n",
        "        self.imputer = DifferentiableImputer(mean_val, median_val)\n",
        "        self.encoder = DifferentiableEncoder(num_categories)\n",
        "        self.model   = SimpleClassifier(input_dim=5)\n",
        "\n",
        "        # Print beta values\n",
        "        print(f\"Initial Encoder beta: {self.encoder.beta}\")\n",
        "        print(f\"Imputer alpha: {self.imputer.alpha}\")\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        # 1) Impute\n",
        "        x_num_imp = self.imputer.impute(x_num)  # shape (N,)\n",
        "\n",
        "        # 2) Encode\n",
        "        x_cat_enc = self.encoder.encode(x_cat)  # shape (N,4)\n",
        "\n",
        "        # 3) Combine\n",
        "        x_full = np.hstack([x_num_imp[:, np.newaxis], x_cat_enc])  # (N,5)\n",
        "\n",
        "        # 4) Predict\n",
        "        return self.model.predict(x_full)\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true, epsilon=1e-10):\n",
        "        # Binary cross-entropy\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def compute_gradients(self, x_full, y_pred, y_true):\n",
        "        # dL/dz = y_pred - y_true  for logistic\n",
        "        error = y_pred - y_true\n",
        "        grad_w = np.dot(x_full.T, error) / len(y_true)  # (input_dim,)\n",
        "        grad_b = np.mean(error)\n",
        "        return grad_w, grad_b\n",
        "\n",
        "    def compute_beta_gradient(self, x_num, x_cat, y_pred, y_true):\n",
        "        # Compute the gradient for beta based on the loss function\n",
        "        # This assumes the loss is differentiable w.r.t. beta.\n",
        "\n",
        "        x_cat_enc = self.encoder.encode(x_cat)\n",
        "        x_full = np.hstack([x_num[:, np.newaxis], x_cat_enc])  # (N, 5)\n",
        "\n",
        "        # Gradient of the loss with respect to beta\n",
        "        # Gradients for beta are derived from the encoding step, and we use the same loss gradient for simplicity.\n",
        "        grad_beta = np.dot((y_pred - y_true), x_cat_enc[:, -1]) / len(y_true)  # Focus on ordinal part\n",
        "\n",
        "        return grad_beta\n",
        "\n",
        "    def sgd_step(self, grad_w, grad_b, grad_beta, lr):\n",
        "        # Update weights and beta parameter\n",
        "        self.model.weights -= lr * grad_w\n",
        "        self.model.bias    -= lr * grad_b\n",
        "        self.encoder.beta   -= lr * grad_beta\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Training loop with SGD for beta first, then for classifier\n",
        "# -----------------------------------------------------------\n",
        "def train_with_sgd(model, x_num_batch, x_cat_batch, y_batch, num_epochs=30, learning_rate=0.01, beta_epochs=1000):\n",
        "    # Ensure x_cat_batch is (N, ) not (N,1)\n",
        "    x_cat_batch = x_cat_batch.ravel()  # Flatten if needed\n",
        "\n",
        "    # First, we optimize beta using SGD for `beta_epochs`\n",
        "    for epoch in range(beta_epochs):\n",
        "        # Forward pass\n",
        "        y_pred = model.forward(x_num_batch, x_cat_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = model.compute_loss(y_pred, y_batch)\n",
        "\n",
        "        # Compute gradients for beta\n",
        "        # Generate x_full for both x_num and x_cat for gradient computation\n",
        "        x_num_imp = model.imputer.impute(x_num_batch)\n",
        "        x_cat_enc = model.encoder.encode(x_cat_batch)\n",
        "        x_full = np.hstack([x_num_imp[:, np.newaxis], x_cat_enc])\n",
        "\n",
        "        # Compute gradients for weights and beta\n",
        "        grad_w, grad_b = model.compute_gradients(x_full, y_pred, y_batch)\n",
        "        grad_beta = model.compute_beta_gradient(x_num_batch, x_cat_batch, y_pred, y_batch)\n",
        "\n",
        "        # Update using SGD\n",
        "        model.sgd_step(grad_w, grad_b, grad_beta, learning_rate)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Beta Optimization Epoch {epoch+1}, Loss = {loss:.4f}, Beta = {model.encoder.beta}\")\n",
        "\n",
        "    # Now optimize the classifier with the optimized beta\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        y_pred = model.forward(x_num_batch, x_cat_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = model.compute_loss(y_pred, y_batch)\n",
        "\n",
        "        # Generate x_full for gradient computation\n",
        "        x_num_imp = model.imputer.impute(x_num_batch)\n",
        "        x_cat_enc = model.encoder.encode(x_cat_batch)\n",
        "        x_full = np.hstack([x_num_imp[:, np.newaxis], x_cat_enc])\n",
        "\n",
        "        # Gradients\n",
        "        grad_w, grad_b = model.compute_gradients(x_full, y_pred, y_batch)\n",
        "\n",
        "        # Update classifier weights\n",
        "        model.sgd_step(grad_w, grad_b, 0, learning_rate)  # No need to update beta here\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Classifier Training Epoch {epoch+1}, Loss = {loss:.4f}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Run it with beta training first\n",
        "# -----------------------------------------------------------\n",
        "pipeline = FullPipeline(mean_val, median_val, num_categories=3)\n",
        "train_with_sgd(pipeline, x_num, x_cat, y, num_epochs=30, learning_rate=0.05, beta_epochs=10)"
      ],
      "metadata": {
        "id": "FsfOUSfmEg3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# non convex with no beta tuning\n",
        "\n",
        "# Non-convex activation function: Swish\n",
        "def swish(x, sigmoid_fn):\n",
        "    return x * sigmoid_fn(x)  # Swish = x * sigmoid(x)\n",
        "\n",
        "# Non-convex loss function: Log-Cosh loss\n",
        "def log_cosh_loss(y_true, y_pred):\n",
        "    return np.mean(np.log(np.cosh(y_pred - y_true)))\n",
        "\n",
        "# Modified SimpleClassifier\n",
        "class NonConvexClassifier:\n",
        "    def __init__(self, input_dim):\n",
        "        self.weights = np.random.randn(input_dim)\n",
        "        self.bias    = np.random.randn(1)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, x):\n",
        "        return swish(np.dot(x, self.weights) + self.bias, self.sigmoid)\n",
        "\n",
        "# Modified FullPipeline\n",
        "class NonConvexPipeline:\n",
        "    def __init__(self, mean_val, median_val, num_categories=3):\n",
        "        self.imputer = DifferentiableImputer(mean_val, median_val)\n",
        "        self.encoder = DifferentiableEncoder(num_categories)\n",
        "        # 1 numeric + 3 one-hot + 1 ordinal = 5\n",
        "        self.model   = NonConvexClassifier(input_dim=5)\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        # 1) Impute\n",
        "        x_num_imp = self.imputer.impute(x_num)  # shape (N,)\n",
        "\n",
        "        # 2) Encode\n",
        "        x_cat_enc = self.encoder.encode(x_cat)  # shape (N,4)\n",
        "\n",
        "        # 3) Combine\n",
        "        x_full = np.hstack([x_num_imp[:, np.newaxis], x_cat_enc])  # (N,5)\n",
        "\n",
        "        # 4) Predict\n",
        "        return self.model.predict(x_full)\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true, epsilon=1e-10):\n",
        "        # Use log-cosh loss for non-convex behavior\n",
        "        return log_cosh_loss(y_true, y_pred)\n",
        "\n",
        "    def compute_gradients(self, x_full, y_pred, y_true):\n",
        "        # Gradient of the log-cosh loss\n",
        "        error = y_pred - y_true\n",
        "        grad_w = np.dot(x_full.T, error) / len(y_true)  # (input_dim,)\n",
        "        grad_b = np.mean(error)\n",
        "        return grad_w, grad_b\n",
        "\n",
        "    def sgd_step(self, grad_w, grad_b, lr):\n",
        "        self.model.weights -= lr * grad_w\n",
        "        self.model.bias    -= lr * grad_b\n",
        "\n",
        "# Training loop remains the same\n",
        "def train_with_sgd(model, x_num_batch, x_cat_batch, y_batch, num_epochs=30, learning_rate=0.01):\n",
        "    # Ensure x_cat_batch is (N, ) not (N,1)\n",
        "    x_cat_batch = x_cat_batch.ravel()  # Flatten if needed\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        y_pred = model.forward(x_num_batch, x_cat_batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = model.compute_loss(y_pred, y_batch)\n",
        "\n",
        "        # We need x_full for gradients:\n",
        "        x_num_imp = model.imputer.impute(x_num_batch)\n",
        "        x_cat_enc = model.encoder.encode(x_cat_batch)\n",
        "        x_full    = np.hstack([x_num_imp[:, np.newaxis], x_cat_enc])\n",
        "\n",
        "        # Gradients\n",
        "        grad_w, grad_b = model.compute_gradients(x_full, y_pred, y_batch)\n",
        "\n",
        "        # Update\n",
        "        model.sgd_step(grad_w, grad_b, learning_rate)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Loss = {loss:.4f}\")\n",
        "\n",
        "# Run with the modified pipeline\n",
        "pipeline = NonConvexPipeline(mean_val, median_val, num_categories=3)\n",
        "train_with_sgd(pipeline, x_num, x_cat, y, num_epochs=30, learning_rate=0.05)"
      ],
      "metadata": {
        "id": "OmWCBvFwEnTj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}